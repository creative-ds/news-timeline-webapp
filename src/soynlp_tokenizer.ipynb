{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization Based on SoyNLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "soynlp 학습 전 기본적인 데이터 전처리\r\n",
    "* 구두점 제거 및 특수문자 제거\r\n",
    "* 모든 sentence를 join하여 하나의 corpus로 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "save_dir = ('../data/조선경제_2019.09.01_2021.08.31.csv')\r\n",
    "df = pd.read_csv(save_dir)\r\n",
    "df['body'].head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    \\n\\n\\n\\n\\n\\n\\n                         1일 오후 1...\n",
       "1    \\n\\n\\n\\n\\n\\n\\n                         80대 노모와...\n",
       "2    \\n\\t\\t\\t\\t\\t\\t\\t\\t.\\n                         ...\n",
       "3    \\n\\t\\t\\t\\t\\t\\t\\t\\t주말과 휴일 동안 강원지역에서 산악사고와 교통사고가...\n",
       "4    \\n\\t\\t\\t\\t\\t\\t\\t\\t제1, 2, 3 석유류 30만ℓ 보관 탱크에 119...\n",
       "Name: body, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df['body_prep'] = df['body'].str.replace(pat=r'[^A-Za-z0-9가-힣]', repl= r' ', regex=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df['body_prep'].head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                                    1일 오후 10시 31분께...\n",
       "1                                    80대 노모와 지체 장애를...\n",
       "2                                                  ...\n",
       "3             주말과 휴일 동안 강원지역에서 산악사고와 교통사고가 잇따랐다    ...\n",
       "4             제1  2  3 석유류 30만  보관 탱크에 119구조대장 기어가 ...\n",
       "Name: body_prep, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "corpus = \"\"\r\n",
    "for body in df['body_prep']:\r\n",
    "    corpus += body"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "corpus_file = open(\"../data/chosun_corpus_0928.txt\", 'w', encoding='utf-8')\r\n",
    "corpus_file.write(corpus)\r\n",
    "corpus_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\r\n",
    "from soynlp.word import WordExtractor\r\n",
    "\r\n",
    "corpus_path = \"../data/chosun_corpus_0928.txt\"\r\n",
    "sents = DoublespaceLineCorpus(corpus_path, iter_sent=True)\r\n",
    "\r\n",
    "\r\n",
    "word_extractor = WordExtractor(min_frequency=100,\r\n",
    "    min_cohesion_forward=0.05, \r\n",
    "    min_right_branching_entropy=0.0\r\n",
    ")\r\n",
    "word_extractor.train(sents) # list of str or like\r\n",
    "words = word_extractor.extract()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training was done. used memory 1.995 Gb\n",
      "all cohesion probabilities was computed. # words = 22273\n",
      "all branching entropies was computed # words = 354305\n",
      "all accessor variety was computed # words = 354305\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from soynlp.tokenizer import LTokenizer\r\n",
    "\r\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in words.items()}\r\n",
    "tokenizer = LTokenizer(scores=cohesion_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df['tokenized'] = [tokenizer.tokenize(body) for body in df['body_prep']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "df.to_csv(\"../data/joongang_accident_token_df.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}